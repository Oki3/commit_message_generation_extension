prompt,generated_message
"
            You are a programmer to produce concise, descriptive commit messages for Git changes.
            Below are up to three examples of commit messages that previously touched upon the same code or files. 
            Please note that the first example is more important and should influence your message the most. 
            Use the style and context of these examples, prioritizing the first examples, to inspire a new commit message for the provided Git diff. 
            Do not include references to issue numbers or pull requests.

            Examples of relevant commit messages:
            1. add more singular exception lists
            2. fix singular *use words

            Now here is the new Git diff for which you must generate a commit message:
            diff --git a/README.md b/README.md
index 0663a29..25657b3 100644
--- a/README.md
+++ b/README.md
@@ -1,137 +1,137 @@
-# Team 3
-Group project (Team 3) for CS4570 Machine Learning for Software Engineering. This repository serves as a basis for research into prompt engineering for automated commit message generation. 
-
-## Requirements
-
-- Python 3.10 or higher
-- [Ollama](https://github.com/jmorganca/ollama) (for model serving)
-- A supported Ollama model (e.g. `mistral`, `codellama`, `phi3.5`)
-
-## Installation
-
-### 1. Install Python and Dependencies
-
-1. **Python**  
-   Make sure you have Python 3.10 or higher installed. If you are on a Unix-like system (Linux/MacOS), run:
-   ```bash
-   python3 --version
-   ```
-   If Python is not installed, visit [python.org](https://www.python.org/downloads/) to download and install it.
-
-2. **Python Dependencies**  
-   Install the required Python libraries using pip:
-   ```bash
-   pip install -r requirements.txt
-   ```
-
-### 2. Install Ollama
-
-Ollama is required to run and manage the models. Download and install Ollama from [ollama.com](https://ollama.com/).
-
-### 3. Install a Model
-
-Ollama requires a model to be installed before you can generate text. Some possible models include:
-- `mistral`
-- `codellama`
-- `phi3.5`
-
-To install a model (for example, `mistral`), run:
-```bash
-ollama pull mistral
-```
-
-## Usage
-- Check `.env` variables are defined
-- Run `python src/main.py`
-- Input the model to use (choose from `deepinfra`, `phi_mini`,`mistral` or `codellama`)
-- See the results in `output/output.csv`, containing the original commit message and the message generated by the model.
-- After obtained outputs from a model, run `python src/evaluate.py` to see the evaluation results.
-
-Before usage make sure Ollama is running using:
-```bash
-ollama serve
-```
-
-Below is an example of how to run the main file:
-```bash
-python src/main.py \
-    --model mistral \
-    --prompt baseline \
-```
-
-## Man page
-
-Below is the placeholder for the detailed manual page of the main file and how to use it:
-
-```
-sage: main.py [-h] [--model {mistral,codellama,phi3.5}] [--prompt {baseline,fewshot,cot}] [--input_size INPUT_SIZE] [--process_amount PROCESS_AMOUNT] [--sequential] [--input_folder INPUT_FOLDER] [--output_folder OUTPUT_FOLDER] [--temperature TEMPERATURE] [--workers WORKERS]
-
-Run one of the experiments with the specified model.
-
-options:
-  -h, --help            show this help message and exit
-  --model {mistral,codellama,phi3.5}
-                        The model to use.
-  --prompt {baseline,fewshot,cot}
-                        The prompt to use.
-  --input_size INPUT_SIZE
-                        The size of the input.
-  --process_amount PROCESS_AMOUNT
-                        The number of items to process.
-  --sequential          Run the experiment sequentially instead of in parallel.
-  --input_folder INPUT_FOLDER
-                        The folder containing the input files.
-  --output_folder OUTPUT_FOLDER
-                        The folder to save the output files.
-  --temperature TEMPERATURE
-                        The temperature for the model generation.
-  --workers WORKERS     The number of workers to use for parallel processing.
-```
-
-## Extension
->Notice: We used WSL2 and macOS as the testing environment, some adaptions for Windows are also implemented. However, they are not tested thoroughly. The current extension can be seen as a Proof of Concept.
-
-To test if the extension is running properly, you need to follow the instructions below:
-
-1. Open `extension/commit-generation` as the root project in Visual Studio Code.
-
-2. Press `F5` to open the visual debugger, then press `Ctrl` + `Shift` + `P`, search and select `Generate commit message` in the list.
-
-3. You should see the process by print statements in the console terminal, ignore the visual studio pop-ups - they are redundant for now.
-
-4. After that you should see the console outputs(below is an example)：
-```
-Initializing virtual environments...
-Virtual environment created. Now installing dependencies...
-Pulling Mistral model via Ollama...
-Dependencies installed. Now fetching git diff...
-Git diff fetched:  Now write diff to temp file...
-Diff file is written to temp file, located at /home/weicheng/.vscode-server/data/User/globalStorage/undefined_publisher.commit-generation/staged_diff.txt , now running the model to generate messages....
-Message automatically copied to the file location: [.../my_messages.txt]
-```
-If you can see the last line, then messages should be copied to the file location, which is typically in the root folder of your repository.
-
-### Model running on the extension
-The extension runs on `src/runExtension.py`. The current model it is running:
-```
-Model: Mistral7b
-Applied technique(s): few-shot
-Temperature: 0.7
-```
-
-To adjust the parameters for the model, modify `src/runExtension.py`. Alternatively, you can put your own model in.
-
-## Other scripts
-
-
-### Input preperation
-Run `prepare_input.py` to transform the dataset `csv` files into input files, by generating the prompts from them. The input files are generated as `{model}_{size}_{technique}.csv`.
-```bash
-python src/prepare_input.py
-```
-
-### Run similar search
-Run `run_similar_search.py` to find similar commits as few shot examples for samples in the dataset. Warning: this script will download full repositories to find similar commits and so will usage a large amount of storage.
-```bash
-python src/few_shot/run_similar_search.py
+# Team 3
+Group project (Team 3) for CS4570 Machine Learning for Software Engineering. This repository serves as a basis for research into prompt engineering for automated commit message generation. 
+
+## Requirements
+
+- Python 3.10 or higher
+- [Ollama](https://github.com/jmorganca/ollama) (for model serving)
+- A supported Ollama model (e.g. `mistral`, `codellama`, `phi3.5`)
+
+## Installation
+
+### 1. Install Python and Dependencies
+
+1. **Python**  
+   Make sure you have Python 3.10 or higher installed. If you are on a Unix-like system (Linux/MacOS), run:
+   ```bash
+   python3 --version
+   ```
+   If Python is not installed, visit [python.org](https://www.python.org/downloads/) to download and install it.
+
+2. **Python Dependencies**  
+   Install the required Python libraries using pip:
+   ```bash
+   pip install -r requirements.txt
+   ```
+
+### 2. Install Ollama
+
+Ollama is required to run and manage the models. Download and install Ollama from [ollama.com](https://ollama.com/).
+
+### 3. Install a Model
+
+Ollama requires a model to be installed before you can generate text. Some possible models include:
+- `mistral`
+- `codellama`
+- `phi3.5`
+
+To install a model (for example, `mistral`), run:
+```bash
+ollama pull mistral
+```
+
+## Usage
+- Check `.env` variables are defined
+- Run `python src/main.py`
+- Input the model to use (choose from `deepinfra`, `phi_mini`,`mistral` or `codellama`)
+- See the results in `output/output.csv`, containing the original commit message and the message generated by the model.
+- After obtained outputs from a model, run `python src/evaluate.py` to see the evaluation results.
+
+Before usage make sure Ollama is running using:
+```bash
+ollama serve
+```
+
+Below is an example of how to run the main file:
+```bash
+python src/main.py \
+    --model mistral \
+    --prompt baseline \
+```
+
+## Man page
+
+Below is the placeholder for the detailed manual page of the main file and how to use it:
+
+```
+sage: main.py [-h] [--model {mistral,codellama,phi3.5}] [--prompt {baseline,fewshot,cot}] [--input_size INPUT_SIZE] [--process_amount PROCESS_AMOUNT] [--sequential] [--input_folder INPUT_FOLDER] [--output_folder OUTPUT_FOLDER] [--temperature TEMPERATURE] [--workers WORKERS]
+
+Run one of the experiments with the specified model.
+
+options:
+  -h, --help            show this help message and exit
+  --model {mistral,codellama,phi3.5}
+                        The model to use.
+  --prompt {baseline,fewshot,cot}
+                        The prompt to use.
+  --input_size INPUT_SIZE
+                        The size of the input.
+  --process_amount PROCESS_AMOUNT
+                        The number of items to process.
+  --sequential          Run the experiment sequentially instead of in parallel.
+  --input_folder INPUT_FOLDER
+                        The folder containing the input files.
+  --output_folder OUTPUT_FOLDER
+                        The folder to save the output files.
+  --temperature TEMPERATURE
+                        The temperature for the model generation.
+  --workers WORKERS     The number of workers to use for parallel processing.
+```
+
+## Extension
+>Notice: We used WSL2 and macOS as the testing environment, some adaptions for Windows are also implemented. However, they are not tested thoroughly. The current extension can be seen as a Proof of Concept.
+
+To test if the extension is running properly, you need to follow the instructions below:
+
+1. Open `extension/commit-generation` as the root project in Visual Studio Code.
+
+2. Press `F5` to open the visual debugger, then press `Ctrl` + `Shift` + `P`, search and select `Generate commit message` in the list.
+
+3. You should see the process by print statements in the console terminal, ignore the visual studio pop-ups - they are redundant for now.
+
+4. After that you should see the console outputs(below is an example)：
+```
+Initializing virtual environments...
+Virtual environment created. Now installing dependencies...
+Pulling Mistral model via Ollama...
+Dependencies installed. Now fetching git diff...
+Git diff fetched:  Now write diff to temp file...
+Diff file is written to temp file, located at /home/weicheng/.vscode-server/data/User/globalStorage/undefined_publisher.commit-generation/staged_diff.txt , now running the model to generate messages....
+Message automatically copied to the file location: [.../my_messages.txt]
+```
+If you can see the last line, then messages should be copied to the file location, which is typically in the root folder of your repository.
+
+### Model running on the extension
+The extension runs on `src/runExtension.py`. The current model it is running:
+```
+Model: Mistral7b
+Applied technique(s): few-shot
+Temperature: 0.7
+```
+
+To adjust the parameters for the model, modify `src/runExtension.py`. Alternatively, you can put your own model in.
+
+## Other scripts
+
+
+### Input preperation
+Run `prepare_input.py` to transform the dataset `csv` files into input files, by generating the prompts from them. The input files are generated as `{model}_{size}_{technique}.csv`.
+```bash
+python src/prepare_input.py
+```
+
+### Run similar search
+Run `run_similar_search.py` to find similar commits as few shot examples for samples in the dataset. Warning: this script will download full repositories to find similar commits and so will usage a large amount of storage.
+```bash
+python src/few_shot/run_similar_search.py
 ```
\ No newline at end of file
diff --git a/src/runExtension.py b/src/runExtension.py
index b881402..158da1a 100644
--- a/src/runExtension.py
+++ b/src/runExtension.py
@@ -1,224 +1,224 @@
-# main_text.py
-
-from concurrent.futures import ThreadPoolExecutor, as_completed
-import os
-import ollama
-from pandas import DataFrame
-import argparse
-import time
-import sys
-
-#from post_processing.post_processing_csv import convert_to_result_file
-#from post_processing.graphs import read_from_files_for_graphs
-#from clean import clean_folder
-
-# --------------------------------------------------------------------
-# Model definitions
-# --------------------------------------------------------------------
-class Model:
-    name: str = """"
-
-    def run(self, prompt: str, temperature: float) -> str:
-        response = ollama.generate(model=self.name, prompt=prompt, options={""temperature"": temperature})
-        return response.response
-
-    def check_installed(self) -> bool:
-        try:
-            ollama.show(self.name)
-            return True
-        except:
-            return False
-
-class MistralModel(Model):
-    name: str = ""mistral""
-
-# --------------------------------------------------------------------
-# TxtExperiment (always Mistral, ""fewshot"")
-# --------------------------------------------------------------------
-class TxtExperiment:
-    model: Model
-    process_amount: int
-    prompt: str
-    # txt_file: str
-    output_csv: str
-    output_txt: str
-    input_df: DataFrame | None = None
-    output_df: DataFrame | None = None
-    start_time: float = 0
-    workers: int
-    temperature: float
-
-    def __init__(
-        self,
-        # txt_file: str,
-        output_csv: str,
-        output_txt: str,
-        process_amount: int,
-        workers: int,
-        temperature: float
-    ):
-        self.model = MistralModel()
-        self.prompt = ""fewshot""
-        
-        # self.txt_file = txt_file
-        self.output_csv = output_csv
-        self.output_txt = output_txt
-        self.process_amount = process_amount
-        self.workers = workers
-        self.temperature = temperature
-
-        # We'll store the input lines in a DataFrame column ""prompt""
-        self.output_df = DataFrame(columns=[""prompt"", ""generated_message""])
-
-    def check_installed(self):
-        if not self.model.check_installed():
-            raise Exception(f""Model {self.model.name} is not installed."")
-
-    def read_input(self):
-        # """"""Read from a single .txt file, one prompt per line.""""""
-        # if not os.path.exists(self.txt_file):
-        #     raise FileNotFoundError(f""TXT file not found: {self.txt_file}"")
-
-        # print(f""Reading TXT: {self.txt_file}"")
-        # with open(self.txt_file, ""r"", encoding=""utf-8"") as f:
-        #     lines = [l.strip() for l in f if l.strip()]
-
-        # self.input_df = DataFrame({""prompt"": lines})
-
-        # # If the requested process_amount > number of lines, clamp it.
-        # if self.process_amount > len(lines):
-        #     self.process_amount = len(lines)
-        print(""Reading from stdin..."")
-        diff_content = sys.stdin.read()
-
-        # Possibly split into lines or treat as one prompt
-        lines = [diff_content]
-        self.input_df = DataFrame({""prompt"": lines})
-        self.process_amount = len(lines)
-
-
-    def save_output_csv(self):
-        """"""Saves all columns (prompt and generated_message) to a CSV file.""""""
-        self.output_df.to_csv(self.output_csv, index=False)
-
-    def save_output_txt(self):
-        """"""Saves only the generated messages to a .txt file (one per line).""""""
-        with open(self.output_txt, ""w"", encoding=""utf-8"") as f:
-            for idx in range(len(self.output_df)):
-                msg = self.output_df.loc[idx, ""generated_message""]
-                # In case of NaN or empty
-                if not isinstance(msg, str):
-                    msg = """"
-                f.write(msg + ""\n"")
-
-    def process_item(self, index: int, temperature: float) -> dict:
-        item = self.input_df.iloc[index]
-        prompt_line = item[""prompt""]
-
-        # We'll combine ""fewshot"" logic however you like;
-        # for now, we just run it as-is with that line as the prompt.
-        # If you had a fewshot template, you'd incorporate it here.
-        generated_message = self.model.run(prompt_line, temperature)
-        return {
-            ""prompt"": prompt_line,
-            ""generated_message"": generated_message
-        }
-
-    def print_progress(self, completed: int, total: int):
-        time_elapsed = time.time() - self.start_time
-        if completed > 0:
-            time_remaining = (time_elapsed / completed) * (total - completed)
-        else:
-            time_remaining = 0
-        print(
-            f""Progress: {completed}/{total} ""
-            f""({(completed/total)*100:.2f}%) done. ""
-            f""Elapsed {time_elapsed:.2f}s, remaining: {time_remaining:.2f}s""
-        )
-
-    def append_result(self, index: int, result: dict):
-        self.output_df.loc[index] = [
-            result[""prompt""],
-            result[""generated_message""]
-        ]
-
-    def append_error(self, index: int):
-        item = self.input_df.iloc[index]
-        self.output_df.loc[index] = [item[""prompt""], """"]
-
-    def start(self):
-        self.start_time = time.time()
-
-    def run_parallel(self):
-        with ThreadPoolExecutor(max_workers=self.workers) as executor:
-            futures = {
-                executor.submit(self.process_item, i, self.temperature): i
-                for i in range(self.process_amount)
-            }
-            total = len(futures)
-            completed = 0
-
-            self.start()
-
-            try:
-                for future in as_completed(futures):
-                    index = futures[future]
-                    try:
-                        result = future.result()
-                        self.append_result(index, result)
-                        completed += 1
-                        self.print_progress(completed, total)
-                    except Exception as e:
-                        print(f""Error processing item {index}: {e}"")
-                        self.append_error(index)
-            except KeyboardInterrupt:
-                print(""Interrupted"")
-                executor.shutdown(wait=False, cancel_futures=True)
-                raise KeyboardInterrupt()
-
-    def run(self):
-        """"""Sequential version.""""""
-        self.start()
-        for i in range(self.process_amount):
-            try:
-                result = self.process_item(i, self.temperature)
-                self.append_result(i, result)
-                self.print_progress(i+1, self.process_amount)
-            except Exception as e:
-                print(f""Error processing item {i}: {e}"")
-                self.append_error(i)
-
-parser = argparse.ArgumentParser(description=""TXT-based experiment (always uses Mistral fewshot)."")
-
-# parser.add_argument(""--txt_file"", type=str, required=True, help=""Path to the .txt file (one prompt per line)."")
-parser.add_argument(""--output_csv"", type=str, default=""output_txt.csv"", help=""Output CSV file for tabular data."")
-parser.add_argument(""--output_txt"", type=str, default=""output_txt.txt"", help=""Output text file with generated lines."")
-parser.add_argument(""--process_amount"", type=int, default=1, help=""Number of lines to process."")
-parser.add_argument(""--workers"", type=int, default=5, help=""Number of parallel workers."")
-parser.add_argument(""--sequential"", action=""store_true"", help=""Run sequentially instead of parallel."")
-parser.add_argument(""--temperature"", type=float, default=0.7, help=""Model generation temperature."")
-
-if __name__ == ""__main__"":
-    args = parser.parse_args()
-
-    experiment = TxtExperiment(
-    #    txt_file=args.txt_file,
-        output_csv=args.output_csv,
-        output_txt=args.output_txt,
-        process_amount=args.process_amount,
-        workers=args.workers,
-        temperature=args.temperature
-    )
-
-    experiment.check_installed()
-    experiment.read_input()
-
-    if args.sequential:
-        experiment.run()
-    else:
-        experiment.run_parallel()
-
-    # Write full table to CSV
-    experiment.save_output_csv()
-    # Write only generated messages to .txt
-    experiment.save_output_txt()
+# main_text.py
+
+from concurrent.futures import ThreadPoolExecutor, as_completed
+import os
+import ollama
+from pandas import DataFrame
+import argparse
+import time
+import sys
+
+#from post_processing.post_processing_csv import convert_to_result_file
+#from post_processing.graphs import read_from_files_for_graphs
+#from clean import clean_folder
+
+# --------------------------------------------------------------------
+# Model definitions
+# --------------------------------------------------------------------
+class Model:
+    name: str = """"
+
+    def run(self, prompt: str, temperature: float) -> str:
+        response = ollama.generate(model=self.name, prompt=prompt, options={""temperature"": temperature})
+        return response.response
+
+    def check_installed(self) -> bool:
+        try:
+            ollama.show(self.name)
+            return True
+        except:
+            return False
+
+class MistralModel(Model):
+    name: str = ""mistral""
+
+# --------------------------------------------------------------------
+# TxtExperiment (always Mistral, ""fewshot"")
+# --------------------------------------------------------------------
+class TxtExperiment:
+    model: Model
+    process_amount: int
+    prompt: str
+    # txt_file: str
+    output_csv: str
+    output_txt: str
+    input_df: DataFrame | None = None
+    output_df: DataFrame | None = None
+    start_time: float = 0
+    workers: int
+    temperature: float
+
+    def __init__(
+        self,
+        # txt_file: str,
+        output_csv: str,
+        output_txt: str,
+        process_amount: int,
+        workers: int,
+        temperature: float
+    ):
+        self.model = MistralModel()
+        self.prompt = ""fewshot""
+        
+        # self.txt_file = txt_file
+        self.output_csv = output_csv
+        self.output_txt = output_txt
+        self.process_amount = process_amount
+        self.workers = workers
+        self.temperature = temperature
+
+        # We'll store the input lines in a DataFrame column ""prompt""
+        self.output_df = DataFrame(columns=[""prompt"", ""generated_message""])
+
+    def check_installed(self):
+        if not self.model.check_installed():
+            raise Exception(f""Model {self.model.name} is not installed."")
+
+    def read_input(self):
+        # """"""Read from a single .txt file, one prompt per line.""""""
+        # if not os.path.exists(self.txt_file):
+        #     raise FileNotFoundError(f""TXT file not found: {self.txt_file}"")
+
+        # print(f""Reading TXT: {self.txt_file}"")
+        # with open(self.txt_file, ""r"", encoding=""utf-8"") as f:
+        #     lines = [l.strip() for l in f if l.strip()]
+
+        # self.input_df = DataFrame({""prompt"": lines})
+
+        # # If the requested process_amount > number of lines, clamp it.
+        # if self.process_amount > len(lines):
+        #     self.process_amount = len(lines)
+        print(""Reading from stdin..."")
+        diff_content = sys.stdin.read()
+
+        # Possibly split into lines or treat as one prompt
+        lines = [diff_content]
+        self.input_df = DataFrame({""prompt"": lines})
+        self.process_amount = len(lines)
+
+
+    def save_output_csv(self):
+        """"""Saves all columns (prompt and generated_message) to a CSV file.""""""
+        self.output_df.to_csv(self.output_csv, index=False)
+
+    def save_output_txt(self):
+        """"""Saves only the generated messages to a .txt file (one per line).""""""
+        with open(self.output_txt, ""w"", encoding=""utf-8"") as f:
+            for idx in range(len(self.output_df)):
+                msg = self.output_df.loc[idx, ""generated_message""]
+                # In case of NaN or empty
+                if not isinstance(msg, str):
+                    msg = """"
+                f.write(msg + ""\n"")
+
+    def process_item(self, index: int, temperature: float) -> dict:
+        item = self.input_df.iloc[index]
+        prompt_line = item[""prompt""]
+
+        # We'll combine ""fewshot"" logic however you like;
+        # for now, we just run it as-is with that line as the prompt.
+        # If you had a fewshot template, you'd incorporate it here.
+        generated_message = self.model.run(prompt_line, temperature)
+        return {
+            ""prompt"": prompt_line,
+            ""generated_message"": generated_message
+        }
+
+    def print_progress(self, completed: int, total: int):
+        time_elapsed = time.time() - self.start_time
+        if completed > 0:
+            time_remaining = (time_elapsed / completed) * (total - completed)
+        else:
+            time_remaining = 0
+        print(
+            f""Progress: {completed}/{total} ""
+            f""({(completed/total)*100:.2f}%) done. ""
+            f""Elapsed {time_elapsed:.2f}s, remaining: {time_remaining:.2f}s""
+        )
+
+    def append_result(self, index: int, result: dict):
+        self.output_df.loc[index] = [
+            result[""prompt""],
+            result[""generated_message""]
+        ]
+
+    def append_error(self, index: int):
+        item = self.input_df.iloc[index]
+        self.output_df.loc[index] = [item[""prompt""], """"]
+
+    def start(self):
+        self.start_time = time.time()
+
+    def run_parallel(self):
+        with ThreadPoolExecutor(max_workers=self.workers) as executor:
+            futures = {
+                executor.submit(self.process_item, i, self.temperature): i
+                for i in range(self.process_amount)
+            }
+            total = len(futures)
+            completed = 0
+
+            self.start()
+
+            try:
+                for future in as_completed(futures):
+                    index = futures[future]
+                    try:
+                        result = future.result()
+                        self.append_result(index, result)
+                        completed += 1
+                        self.print_progress(completed, total)
+                    except Exception as e:
+                        print(f""Error processing item {index}: {e}"")
+                        self.append_error(index)
+            except KeyboardInterrupt:
+                print(""Interrupted"")
+                executor.shutdown(wait=False, cancel_futures=True)
+                raise KeyboardInterrupt()
+
+    def run(self):
+        """"""Sequential version.""""""
+        self.start()
+        for i in range(self.process_amount):
+            try:
+                result = self.process_item(i, self.temperature)
+                self.append_result(i, result)
+                self.print_progress(i+1, self.process_amount)
+            except Exception as e:
+                print(f""Error processing item {i}: {e}"")
+                self.append_error(i)
+
+parser = argparse.ArgumentParser(description=""TXT-based experiment (always uses Mistral fewshot)."")
+
+# parser.add_argument(""--txt_file"", type=str, required=True, help=""Path to the .txt file (one prompt per line)."")
+parser.add_argument(""--output_csv"", type=str, default=""output_txt.csv"", help=""Output CSV file for tabular data."")
+parser.add_argument(""--output_txt"", type=str, default=""output_txt.txt"", help=""Output text file with generated lines."")
+parser.add_argument(""--process_amount"", type=int, default=1, help=""Number of lines to process."")
+parser.add_argument(""--workers"", type=int, default=5, help=""Number of parallel workers."")
+parser.add_argument(""--sequential"", action=""store_true"", help=""Run sequentially instead of parallel."")
+parser.add_argument(""--temperature"", type=float, default=0.7, help=""Model generation temperature."")
+
+if __name__ == ""__main__"":
+    args = parser.parse_args()
+
+    experiment = TxtExperiment(
+    #    txt_file=args.txt_file,
+        output_csv=args.output_csv,
+        output_txt=args.output_txt,
+        process_amount=args.process_amount,
+        workers=args.workers,
+        temperature=args.temperature
+    )
+
+    experiment.check_installed()
+    experiment.read_input()
+
+    if args.sequential:
+        experiment.run()
+    else:
+        experiment.run_parallel()
+
+    # Write full table to CSV
+    experiment.save_output_csv()
+    # Write only generated messages to .txt
+    experiment.save_output_txt()

            Format:
            A short commit message (in one sentence) describing what changed and why, consistent with the style 
            and context demonstrated by the above examples.

            Output:"," Added support for processing a text file in parallel with multiple workers using ThreadPoolExecutor. The user can specify the number of workers via the command-line argument '--workers'. If not specified, the default value is 5."
